dataset$Level3 = dataset$Level^3
dataset$Level4 = dataset$Level^4
data<-iris
str(data)
summary(data)
library(caTools)
set.seed(123)
library(caTools)
install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(data$Species, SplitRatio = 0.75)
training_set = subset(data, split == TRUE)
test_set = subset(data, split == FALSE)
library(e1071)
classifier = svm(formula = Species ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
y_pred = predict(classifier, newdata = test_set[-5])
plot(classifier, training_set, petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, test_set, petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, training_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, test_set, petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, test_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
cm = table(test_set[,5], y_pred)
cm
getwd()
concrete = read.csv("Datasets/Concrete_Data")
concrete = read.csv("Datasets\Concrete_Data")
getwd()
concrete = read.csv("Datasets\Concrete_Data")
concrete <- read.csv("Datasets/Concrete_Data.csv")
View(concrete)
View(concrete)
str(concrete)
# total 9 variables are there. one is 'strength' dependent on all the
# other 8 variables
# Neural networks work best when the input data are scaled to a
#narrow range around zero, and here, we see values ranging anywhere from
# up to over a thousand. So normalization th data
hist(concrete$strength)
normalize<- function(x) { return ((x - min(x)) / (max(x) - min(x)))}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
summary(concrete_norm$strength)
summary(concrete$strength)
concrete_train<- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
library(neuralnet)
concrete_model <- neuralnet(strength~cement + slag + ash + water
+ superplasticizer
+ coarseagg + fineagg + age,
data = concrete_train)
model_result = compute(concrete_model, concrete_test[1:8])
# it returns the list with 2 components: $neurons, which stores the
# neurons for each layer in the network, and
# $net.result, which stores the predicted values.
predicted_strength <- model_results$net.result
model_results = compute(concrete_model, concrete_test[1:8])
# it returns the list with 2 components: $neurons, which stores the
# neurons for each layer in the network, and
# $net.result, which stores the predicted values.
predicted_strength <- model_results$net.result
cor(predicted_strength, concrete_test$strength)
concrete_model2 <- neuralnet(strength~cement + slag + ash + water
+ superplasticizer
+ coarseagg + fineagg + age,
data = concrete_train, hidden = 5)
plot(concrete_model2)
model_results2 = compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
dataset<-read.csv("Datasets/social.csv")
View(dataset)
library(caTools)
str(dataset)
dataset = dataset[3:5]
dataset$Purchased = factor(dataset$Purchased, levels=c(0,1))
set.seed(123)
dataset <- read.csv("social.csv")
dataset<-read.csv("Datasets/social.csv")
str(dataset)
dataset= dataset[3:5]
View(dataset)
dataset$Purchased <- factor(dataset$Purchased, levels=c(0,1))
install.packages('caTools')
install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set= subset(dataset,split==TRUE)
test_set <- subset(dataset,split==FALSE)
#normalization
training_set[-3] <- scale(training_set[-3])
View(training_set)
test_set[-3] = scale(test_set[-3])
library(e1073)
library(e1071)
classifier = svm(formula = purchased~,
classifier = svm(formula = purchased~.,
data = training_set,
type = 'c=classification',
kernel = 'linear')
classifier = svm(formula = purchased~.,
data = training_set,
type = 'c=classification',
kernel = 'linear')
classifier = svm(formula = purchased~.,
data = training_set,
type = 'c-classification',
kernel = 'linear')
classifier = svm(formula = Purchased~.,
data = training_set,
type = 'C-classification',
kernel = 'linear')
#Setting the path of directory
setwd("E:/Study/LPU/B.TECH/4th Year/7th Semester/INT234 - PREDICTIVE ANALYTICS/PREDICTIVE-ANALYTICS/R codes")
df<- read.csv("Datasets/sales dataset.csv")
View(df)
summary(df)
# Boxplot
library(ggplot2)
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Cost))+geom_boxplot()
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Qty))+geom_boxplot()
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Cost))+geom_boxplot()
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Qty))+geom_boxplot()
#Preprocessing the sales data
df <- read.csv("Datasets/SalesDatafor preprocessing.csv")
str(df)
summary(df)
# Delete all rows with missing data and name the dataset as df1
df1 <- na.omit(df)
summary(df1)
# Replace the missing values with the mean value of each variable
df$Sales[is.na(df$Sales)] <- mean(df$Sales, na.rm = TRUE)
df$Profit[is.na(df$Profit)] <- mean(df$Profit, na.rm = TRUE)
df$Unit.Price[is.na(df$Unit.Price)] <- mean(df$Unit.Price, na.rm = TRUE)
summary(df)
df$Order.Priority[is.na(df$Order.Priority)] <- sample(levels(df$Order.Priority),
size = sum(is.na(df$Order.Priority)),
replace = TRUE)
# Replace the missing values with random value between min and max of each variable
df$Sales[is.na(df$Sales)] <- runif(sum(is.na(df$Sales)), min(df$Sales, na.rm = TRUE), max(df$Sales, na.rm = TRUE))
#Preprocessing the sales data
df <- read.csv("Datasets/SalesDatafor preprocessing.csv")
View(df)
str(df)
summary(df)
# Delete all rows with missing data and name the dataset as df1
df1 <- na.omit(df)
summary(df1)
# Replace the missing values with the mean value of each variable
df$Sales[is.na(df$Sales)] <- mean(df$Sales, na.rm = TRUE)
df$Profit[is.na(df$Profit)] <- mean(df$Profit, na.rm = TRUE)
df$Unit.Price[is.na(df$Unit.Price)] <- mean(df$Unit.Price, na.rm = TRUE)
summary(df)
df$Order.Priority[is.na(df$Order.Priority)] <- sample(levels(df$Order.Priority),
size = sum(is.na(df$Order.Priority)),
replace = TRUE)
# Load necessary libraries
library(dplyr)  # for data manipulation
library(tidyr)  # for data tidying
library(caret)  # for classification
library(caretEnsemble)  # for regression and classification
library(cluster)  # for clustering
# Load your dataset (replace 'your_dataset.csv' with the actual file path or data frame)
your_data <- read.csv("your_dataset.csv")
# Load necessary libraries
library(dplyr)  # for data manipulation
library(tidyr)  # for data tidying
library(caret)  # for classification
library(caretEnsemble)  # for regression and classification
library(cluster)  # for clustering
# Load your dataset (replace 'your_dataset.csv' with the actual file path or data frame)
your_data <- read.csv("your_dataset.csv")
# Classification Example
# Assuming you have a classification target variable named 'class_target'
# Split data into training and testing sets (70-30 split)
set.seed(123)  # for reproducibility
train_index <- createDataPartition(your_data$class_target, p = 0.7, list = FALSE)
train_data <- your_data[train_index, ]
test_data <- your_data[-train_index, ]
# Train a classification model (e.g., Random Forest)
rf_model <- train(class_target ~ ., data = train_data, method = "rf")
# Make predictions
predictions <- predict(rf_model, test_data)
# Evaluate the classification model (e.g., confusion matrix and accuracy)
confusionMatrix(predictions, test_data$class_target)
# Clustering Example
# Assuming you want to perform k-means clustering on your dataset
# Select the columns for clustering (replace 'columns_for_clustering')
cluster_data <- your_data[, c('columns_for_clustering')]
# Perform k-means clustering with a specified number of clusters (e.g., 3)
kmeans_model <- kmeans(cluster_data, centers = 3)
# View cluster assignments
cluster_assignments <- kmeans_model$cluster
# Save the cluster assignments back to your data
your_data$cluster_group <- cluster_assignments
# Save the preprocessed dataset (optional)
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
df <- iris
head(df)
str(df)
summary(df)
View(df)
data("iris")
View(airquality)
head(iris)
summary(iris)
#NAs
colSums(is.na(iris))
df = iris %>% select(1:4) %>% scale() %>% data.frame()
#NAs
colSums(is.na(iris))
df = iris %>% select(1:4) %>% scale() %>% data.frame()
library(dplyr)
6df = iris %>% select(1:4) %>% scale() %>% data.frame()
6df = iris %>% select(1:4) %>% scale() %>% data.frame()
df = iris %>% select(1:4) %>% scale() %>% data.frame()
head(df)
result = kmeans(df, 3, nstart = 25)
print(result)
aggregate(df, by = list(cluster = result$cluster),mean)
#df = cbind(df, iris$Species)
newdata = cbind(df, cluster=result$cluster)
head(newdata)
result$cluster
head(result$cluster,4)
result$centers
result$size
#visulization
library("ggplot2")
library(factoextra)
ggplot(data = newdata, aes(x=result$cluster)) + geom_bar(fill = "steelblue")
library(factoextra)
ggplot(data = newdata, aes(x=result$cluster)) + geom_bar(fill = "steelblue")
#Cluster
fviz_cluster(result, data = newdata)
View(iris)
sum(is.na(iris))
library("ggplot2")
ggplot(iris, aes(x=Petal.Length,y=Petal.Width,color=Species)) +geom_point()
?kmeans
result<-kmeans(iris[,3:4],centers = 3,nstart = 25)
print(result)
result$cluster
head(result$cluster, 4)
result$size
result$centers
data("iris")
str(iris)
iris.new<- iris[,c(1,2,3,4)]
iris.class<- iris[,"Species"]
head(iris.new)
head(iris.class)
normalize <- function(x){
return ((x-min(x))/(max(x)-min(x)))
}
iris.new$Sepal.Length<- normalize(iris.new$Sepal.Length)
iris.new$Sepal.Width<- normalize(iris.new$Sepal.Width)
iris.new$Petal.Length<- normalize(iris.new$Petal.Length)
iris.new$Petal.Width<- normalize(iris.new$Petal.Width)
head(iris.new)
result<- kmeans(iris.new,3)
result$size
table(result$cluster,iris.class)
#install.packages("factoextra")
library("factoextra")
#install.packages("NbClust")
#library("NbClust")
fviz_nbclust(iris[,3:4],kmeans,method = "wss") +
geom_vline(xintercept = 3,linetype=5) + labs(subtitle="Elbow method")
#visulization
library("ggplot2")
library(factoextra)
ggplot(data = newdata, aes(x=result$cluster)) + geom_bar(fill = "steelblue")
#Cluster
fviz_cluster(result, data = newdata)
iris
iris1=iris1iris1
iris1=iris1
iris
iris
iris1=iris
d = dist(iris1, method = "euclidean")
iris1
d = dist(iris1, method = "euclidean")
iris1$Species = NULL
iris1
d = dist(iris1, method = "euclidean")
hclust?
hfit = hclust(d, method = "average")
plot(hfit)
hclust?
hfit = hclust(d, method = "average")
?hclust
hfit = hclust(d, method = "average")
plot(hfit)
cutree?
grps = cutree(hfit,k=4)
?cutree
grps = cutree(hfit,k=4)
grps
rect.hclust(hfit,k=4,border="red")
#start
install.packages("arulesViz")
install.packages("arules")
install.packages("arules")
library(datasets)
library(arules)
library(arules)
library(arulesViz)
#explore the data
data(Groceries)
class(Groceries)
Groceries@itemInfo[1:20,]
View(Groceries)
apply(Groceries@data[,10:20],2,function(r) paste(Groceries@itemInfo[r,"labels"]
, collapse=", "))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
summary(itemsets)
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target= "frequent itemsets"))
library(datasets)
library(arules)
library(arulesViz)
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,
target= "frequent itemsets"))
summary(itemsets)
#explore the data
data(Groceries)
class(Groceries)
Groceries@itemInfo[1:20,]
View(Groceries)
apply(Groceries@data[,10:20],2,function(r) paste(Groceries@itemInfo[r,"labels"]
, collapse=", "))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,
target= "frequent itemsets"))
summary(itemsets)
inspect(head(sort(itemsets,by="support"),10))
itemsets<-apriori(Groceries,parameter=list(minlen=2,maxlen=2,support=0.02,
target="frequent itemsets"))
#start
install.packages("arulesViz")
install.packages("arules")
install.packages("arules")
install.packages("arules")
install.packages("arules")
sms_raw<- read.csv("Datasets\sms_spam")
sms_raw<- read.csv("Datasets/sms_spam")
sms_raw<- read.csv("Datasets/sms_spam.csv")
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$text)
#choose sms_spam dataset
sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
#choose sms_spam dataset
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
sms_raw<-read.csv("Datasets/sms_spam")
#choose sms_spam dataset
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
sms_raw<-read.csv("Datasets/sms_spam")
#choose sms_spam dataset
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
sms_raw<-read.csv("Datasets/sms_spam")
#choose sms_spam dataset
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
df<- read.csv("Datasets/sms_spam")
#choose sms_spam dataset
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
getwd()
df<- read.csv("Datasets/sms_spam.csv")
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
#install.packages("tm")
library(tm)
install.packages("tm")
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
df<- read.csv("Datasets/sms_spam.csv")
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
#install.packages("tm")
library(tm)
# First step for text mining is to create corpus,
#which is the collection of text documents
# types of corpus - Vcorpus, Pcorpus
#VectorCorpus reader function which used to create source object
#from existing vector
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
inspect(sms_corpus[1:2])
as.character(sms_corpus[[1]])
lapply(sms_corpus[1:2], as.character)
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
sms_corpus_clean
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus_clean,
removeWords, stopwords())
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
#install.packages("SnowballC")
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
View(sms_dtm)
#######word cloud chart######
#install.packages("wordcloud")
library("wordcloud")
#######word cloud chart######
#install.packages("wordcloud")
library("wordcloud")
wordcloud(sms_corpus_clean,min.freq = 150,random.order = F)
#install.packages("RColorBrewer")
library("RColorBrewer")
wordcloud(sms_corpus_clean,min.freq = 150,random.order = F,
color=brewer.pal(8,"Dark2"),rot.per = 0.20)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
#install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
findFreqTerms(sms_dtm_train, 5)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2,
convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2,
convert_counts)
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
wordcloud(sms_corpus_clean,min.freq = 150,random.order = F,
color=brewer.pal(8,"Dark2"),rot.per = 0.20)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
#install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
findFreqTerms(sms_dtm_train, 5)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2,
convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2,
convert_counts)
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
