library(class)
wbcd_test_pred <- knn(tarin=wbcd_train,test = wbcd_test,
cl = wbcd_train_labels, k=21)
install.packages("gmodels")
library(gmodels)
CrossTable()
library(caret)
getwd()
setwd("E:/Study/LPU/B.TECH/4th Year/7th Semester/INT234 - PREDICTIVE ANALYTICS/PREDICTIVE-ANALYTICS/R codes")
wbcd<- read.csv("Datasets/wisc_bc_data.csv")
View(wbcd)
getwd()
setwd("E:/Study/LPU/B.TECH/4th Year/7th Semester/INT234 - PREDICTIVE ANALYTICS/PREDICTIVE-ANALYTICS/R codes")
wbcd<- read.csv("Datasets/wisc_bc_data.csv")
summary(wbcd)
str(wbcd)
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B","M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis))*100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize<-function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
wbcd_train<-wbcd_n[1:469, ]
wbcd_test<-wbcd_n[470:569, ]
wbcd_train_labels<-wbcd[1:469, 1]
wbcdd_test_labels<-wbcd[470:569, 1]
install.packages("class")
library(class)
wbcd_test_pred <- knn(tarin=wbcd_train,test = wbcd_test,
cl = wbcd_train_labels, k=21)
wbcd_test_labels<-wbcd[470:569, 1]
install.packages("class")
library(class)
install.packages("class")
install.packages("class")
wbcd_test_pred <- knn(train=wbcd_train,test = wbcd_test,
cl = wbcd_train_labels, k=21)
library(gmodels)
install.packages("class")
library(class)
wbcd_test_pred <- knn(train=wbcd_train,test = wbcd_test,
cl = wbcd_train_labels, k=21)
install.packages("gmodels")
install.packages("gmodels")
CrossTable(x = wbcd_test_labels,y=wbcd_test_pred,
prop.chisq=FALSE)
aa<-table(wbcdd_test_labels,wbcd_test_pred)
library(caret)
confusionMatrix(aa)
getwd()
setwd("E:/Study/LPU/B.TECH/4th Year/7th Semester/INT234 - PREDICTIVE ANALYTICS/PREDICTIVE-ANALYTICS/R codes")
wbcd<- read.csv("Datasets/wisc_bc_data.csv")
View(wbcd)
summary(wbcd)
str(wbcd)
wbcd <- wbcd[-1]
View(wbcd)
summary(wbcd)
str(wbcd)
wbcd <- wbcd[-1]
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B","M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis))*100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize<-function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
wbcd_train<-wbcd_n[1:469, ]
wbcd_test<-wbcd_n[470:569, ]
wbcd_train_labels<-wbcd[1:469, 1]
getwd()
setwd("E:/Study/LPU/B.TECH/4th Year/7th Semester/INT234 - PREDICTIVE ANALYTICS/PREDICTIVE-ANALYTICS/R codes")
wbcd<- read.csv("Datasets/wisc_bc_data.csv")
summary(wbcd)
str(wbcd)
wbcd <- wbcd[-1]
table(wbcd$diagnosis)
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B","M"),
labels = c("Benign", "Malignant"))
round(prop.table(table(wbcd$diagnosis))*100, digits = 1)
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
normalize<-function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_n$area_mean)
wbcd_train<-wbcd_n[1:469, ]
wbcd_test<-wbcd_n[470:569, ]
wbcd_train_labels<-wbcd[1:469, 1]
wbcd_test_labels<-wbcd[470:569, 1]
library(class)
wbcd_test_pred <- knn(train=wbcd_train,test = wbcd_test,
cl = wbcd_train_labels, k=21)
library(gmodels)
CrossTable(x = wbcd_test_labels,y=wbcd_test_pred,
prop.chisq=FALSE)
aa<-table(wbcdd_test_labels,wbcd_test_pred)
library(caret)
confusionMatrix(aa)
getwd()
??chickmik
??chickwts
df <- chickwts
View(df)
summary(df)
str(df)
??diamonds
??chickwts
df <- chickwts
summary(df)
str(df)
View(df)
ggplot(data = df, aes(x=factor(df$feed),y=df$weight, fill=factor(feed)))+
geom_boxplot()
df <- ChickWeight
View(df)
summary(df)
str(df)
ggplot(data = df, aes(x=factor(df$feed),y=df$weight, fill=factor(feed)))+
geom_boxplot()
ggplot(data = df, aes(x=factor(df$Chick),y=df$weight, fill=factor(Chick)))+
geom_boxplot()
getwd()
setwd("E:/Study/LPU/B.TECH/4th Year/7th Semester/INT234 - PREDICTIVE ANALYTICS/PREDICTIVE-ANALYTICS/R codes")
df <- read.csv("Datasets/student_data.csv")
View(df)
str(data)
getwd()
setwd("E:/Study/LPU/B.TECH/4th Year/7th Semester/INT234 - PREDICTIVE ANALYTICS/PREDICTIVE-ANALYTICS/R codes")
data <- read.csv("Datasets/student_data.csv")
View(data)
str(data)
library(naivebayes)
install.packages("naivebayes")
library(naivebayes)
library(naivebayes)
library(ggplot2)
library(psych)
install.packages("psych")
library(dplyr)
data <- read.csv("Datasets/student_data.csv")
View(data)
str(data)
summary(data)
??naivebayes
install.packages("e1071")
install.packages("naivebayes")
install.packages("psych")
library(naivebayes)
library(ggplot2)
library(psych)
library(dplyr)
install.packages("dplyr")
install.packages("dplyr")
install.packages("naivebayes")
library(dplyr)
data$rank <-as.factor(data$rank)
data$admit<- as.factor(data$admit)
str(data)
xtabs(~admit+rank,data=data)
pairs.panels(data[-1])
data %>% ggplot(aes(x=admit,y=gre,fill=admit)) + geom_boxplot()+ggtitle("Box Plot")
data$rank <-as.factor(data$rank)
data$admit<- as.factor(data$admit)
str(data)
xtabs(~admit+rank,data=data)
pairs.panels(data[-1])
data %>% ggplot(aes(x=admit,y=gre,fill=admit)) + geom_boxplot()+ggtitle("Box Plot")
data %>% ggplot(aes(x=gpa , fill=admit))+geom_boxplot() + ggtitle("BoxÂ Plot")
data<-read.csv(file.choose(), header = T, stringsAsFactors = T)
?attach
attach(data)
View(data)
str(data)
summary(data)
data<- data[,-1]
?lm
?abline
# Handling the NA's
data$LungCap[is.na(data$LungCap)] <- mean(data$LungCap, na.rm = T)
data$Age[is.na(data$Age)] <- mean(data$Age, na.rm = T)
data$Height[is.na(data$Height)] <- mean(data$Height, na.rm = T)
# NOrmalization
nor <- function(x) { (x = min(x)) / (max(x)-min(x))}
norm <- as.data.frame(lapply(data[, c(1, 2, 3)], nor))
summary(norm)
plot(Age, LungCap, main = "scaatterplot")
model <- lm(LungCap-Age)
model <- lm(LungCap-Age)
install.packages("lm")
model <- lm(LungCap-Age)
model <- lm(LungCap~Age)
summary(model)
attributes(model)
abline(model,col = 2, lwd = 3)
checklungs = data.frame(Age = 10)
result = predict(model, checklungs)
result
# Choose builtin airquality dataset
data<- airquality
?attach
attach(data)
View(data)
str(data)
summary(data)
library(ggplot2)
data("airquality")
View(airquality)
library(ggplot2)
data("airquality")
View(airquality)
aq <- airquality
summary(aq)
str(aq)
#Handling NA's
aq$Ozone[is.na(aq$Ozone)]<-mean(aq$Ozone, na.rm = T)
aq$Solar.R[is.na(aq$Solar.R)]<-mean(aq$Solar.R,na.rm = T)
#Normalization
nor <- function(x){(x-min(x)) / (max(x)-min(x)) }
aq_norm < -as.data.frame(lapply(aq[c(1,2,3,4)], nor))
aq[c(1,2,3,4)] <- aq_norm
attach(aq)
library(ggplot2)
data("airquality")
aq <- airquality
summary(aq)
str(aq)
#Handling NA's
aq$Ozone[is.na(aq$Ozone)]<-mean(aq$Ozone, na.rm = T)
aq$Solar.R[is.na(aq$Solar.R)]<-mean(aq$Solar.R,na.rm = T)
#Normalization
nor <- function(x){(x-min(x)) / (max(x)-min(x)) }
aq_norm < -as.data.frame(lapply(aq[c(1,2,3,4)], nor))
aq_norm <- as.data.frame(lapply(aq[c(1,2,3,4)], nor))
aq[c(1,2,3,4)] <- aq_norm
attach(aq)
attach(aq)
#model
MULTIREG <- lm(Ozone~Solar.R + Wind + Temp)
summary(MULTIREG)
attributes(MULTIREG)
#test data
checkozone = data.frame(Solar.R = 150, Wind = 9, Temp =80)
#Predict
result = predict(MULTIREG, checkozone)
result
##Visualization of model
plot(Ozone ~ MULTIREG$fit)
plot(MULTIREG$fit + MULTIREG$res ~ MULTIREG$fit)
abline(0,1,col=5, lwd=3) #0 is the intercept and 1 is the slope.
?abline
##Visualization of model
plot(Ozone ~ MULTIREG$fit)
plot(MULTIREG$fit + MULTIREG$res ~ MULTIREG$fit)
abline(0,1,col=5, lwd=3) #0 is the intercept and 1 is the slope.
#fit - Fitted Values, also known as predicted Values, are the value that a
#fit - Fitted Values, also known as predicted Values, are the value that a
#regression predicts for the dependent Variable(or response Variable) based on the
#res - Residuals,  also known as errors, are the difference between the
#res - Residuals,  also known as errors, are the difference between the
#observed values of the dependant variables and the corresponding predicted
#res - Residuals,  also known as errors, are the difference between the
#observed values of the dependant variables and the corresponding predicted
#(fitted) values from the regression model.
#res - Residuals,  also known as errors, are the difference between the
#observed values of the dependant variables and the corresponding predicted
#(fitted) values from the regression model.
#res - Residuals,  also known as errors, are the difference between the
#observed values of the dependant variables and the corresponding predicted
#(fitted) values from the regression model.
#res - Residuals,  also known as errors, are the difference between the
#observed values of the dependant variables and the corresponding predicted
#(fitted) values from the regression model.
#Polynomial Regression
dataset <- read.csv(file.choose(),sep = ',',header = T)
dataset = dataset[2:3]
dataset
#Fitting Linear Regression to the dataset
lin_reg = lm(formula = Salary ~ .,
data = dataset)
summary(lin_reg)
dataset$Level2 = dataset$Level^2
dataset$Level3 = dataset$Level^3
dataset$Level4 = dataset$Level^4
data<-iris
str(data)
summary(data)
library(caTools)
set.seed(123)
library(caTools)
install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(data$Species, SplitRatio = 0.75)
training_set = subset(data, split == TRUE)
test_set = subset(data, split == FALSE)
library(e1071)
classifier = svm(formula = Species ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
y_pred = predict(classifier, newdata = test_set[-5])
plot(classifier, training_set, petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, test_set, petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, training_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, test_set, petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, test_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
cm = table(test_set[,5], y_pred)
cm
getwd()
concrete = read.csv("Datasets/Concrete_Data")
concrete = read.csv("Datasets\Concrete_Data")
getwd()
concrete = read.csv("Datasets\Concrete_Data")
concrete <- read.csv("Datasets/Concrete_Data.csv")
View(concrete)
View(concrete)
str(concrete)
# total 9 variables are there. one is 'strength' dependent on all the
# other 8 variables
# Neural networks work best when the input data are scaled to a
#narrow range around zero, and here, we see values ranging anywhere from
# up to over a thousand. So normalization th data
hist(concrete$strength)
normalize<- function(x) { return ((x - min(x)) / (max(x) - min(x)))}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
summary(concrete_norm$strength)
summary(concrete$strength)
concrete_train<- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
library(neuralnet)
concrete_model <- neuralnet(strength~cement + slag + ash + water
+ superplasticizer
+ coarseagg + fineagg + age,
data = concrete_train)
model_result = compute(concrete_model, concrete_test[1:8])
# it returns the list with 2 components: $neurons, which stores the
# neurons for each layer in the network, and
# $net.result, which stores the predicted values.
predicted_strength <- model_results$net.result
model_results = compute(concrete_model, concrete_test[1:8])
# it returns the list with 2 components: $neurons, which stores the
# neurons for each layer in the network, and
# $net.result, which stores the predicted values.
predicted_strength <- model_results$net.result
cor(predicted_strength, concrete_test$strength)
concrete_model2 <- neuralnet(strength~cement + slag + ash + water
+ superplasticizer
+ coarseagg + fineagg + age,
data = concrete_train, hidden = 5)
plot(concrete_model2)
model_results2 = compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
dataset<-read.csv("Datasets/social.csv")
View(dataset)
library(caTools)
str(dataset)
dataset = dataset[3:5]
dataset$Purchased = factor(dataset$Purchased, levels=c(0,1))
set.seed(123)
dataset <- read.csv("social.csv")
dataset<-read.csv("Datasets/social.csv")
str(dataset)
dataset= dataset[3:5]
View(dataset)
dataset$Purchased <- factor(dataset$Purchased, levels=c(0,1))
install.packages('caTools')
install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set= subset(dataset,split==TRUE)
test_set <- subset(dataset,split==FALSE)
#normalization
training_set[-3] <- scale(training_set[-3])
View(training_set)
test_set[-3] = scale(test_set[-3])
library(e1073)
library(e1071)
classifier = svm(formula = purchased~,
classifier = svm(formula = purchased~.,
data = training_set,
type = 'c=classification',
kernel = 'linear')
classifier = svm(formula = purchased~.,
data = training_set,
type = 'c=classification',
kernel = 'linear')
classifier = svm(formula = purchased~.,
data = training_set,
type = 'c-classification',
kernel = 'linear')
classifier = svm(formula = Purchased~.,
data = training_set,
type = 'C-classification',
kernel = 'linear')
#Setting the path of directory
setwd("E:/Study/LPU/B.TECH/4th Year/7th Semester/INT234 - PREDICTIVE ANALYTICS/PREDICTIVE-ANALYTICS/R codes")
df<- read.csv("Datasets/sales dataset.csv")
View(df)
summary(df)
# Boxplot
library(ggplot2)
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Cost))+geom_boxplot()
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Qty))+geom_boxplot()
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Cost))+geom_boxplot()
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Qty))+geom_boxplot()
#Preprocessing the sales data
df <- read.csv("Datasets/SalesDatafor preprocessing.csv")
str(df)
summary(df)
# Delete all rows with missing data and name the dataset as df1
df1 <- na.omit(df)
summary(df1)
# Replace the missing values with the mean value of each variable
df$Sales[is.na(df$Sales)] <- mean(df$Sales, na.rm = TRUE)
df$Profit[is.na(df$Profit)] <- mean(df$Profit, na.rm = TRUE)
df$Unit.Price[is.na(df$Unit.Price)] <- mean(df$Unit.Price, na.rm = TRUE)
summary(df)
df$Order.Priority[is.na(df$Order.Priority)] <- sample(levels(df$Order.Priority),
size = sum(is.na(df$Order.Priority)),
replace = TRUE)
# Replace the missing values with random value between min and max of each variable
df$Sales[is.na(df$Sales)] <- runif(sum(is.na(df$Sales)), min(df$Sales, na.rm = TRUE), max(df$Sales, na.rm = TRUE))
#Preprocessing the sales data
df <- read.csv("Datasets/SalesDatafor preprocessing.csv")
View(df)
str(df)
summary(df)
# Delete all rows with missing data and name the dataset as df1
df1 <- na.omit(df)
summary(df1)
# Replace the missing values with the mean value of each variable
df$Sales[is.na(df$Sales)] <- mean(df$Sales, na.rm = TRUE)
df$Profit[is.na(df$Profit)] <- mean(df$Profit, na.rm = TRUE)
df$Unit.Price[is.na(df$Unit.Price)] <- mean(df$Unit.Price, na.rm = TRUE)
summary(df)
df$Order.Priority[is.na(df$Order.Priority)] <- sample(levels(df$Order.Priority),
size = sum(is.na(df$Order.Priority)),
replace = TRUE)
# Load necessary libraries
library(dplyr)  # for data manipulation
library(tidyr)  # for data tidying
library(caret)  # for classification
library(caretEnsemble)  # for regression and classification
library(cluster)  # for clustering
# Load your dataset (replace 'your_dataset.csv' with the actual file path or data frame)
your_data <- read.csv("your_dataset.csv")
# Load necessary libraries
library(dplyr)  # for data manipulation
library(tidyr)  # for data tidying
library(caret)  # for classification
library(caretEnsemble)  # for regression and classification
library(cluster)  # for clustering
# Load your dataset (replace 'your_dataset.csv' with the actual file path or data frame)
your_data <- read.csv("your_dataset.csv")
# Classification Example
# Assuming you have a classification target variable named 'class_target'
# Split data into training and testing sets (70-30 split)
set.seed(123)  # for reproducibility
train_index <- createDataPartition(your_data$class_target, p = 0.7, list = FALSE)
train_data <- your_data[train_index, ]
test_data <- your_data[-train_index, ]
# Train a classification model (e.g., Random Forest)
rf_model <- train(class_target ~ ., data = train_data, method = "rf")
# Make predictions
predictions <- predict(rf_model, test_data)
# Evaluate the classification model (e.g., confusion matrix and accuracy)
confusionMatrix(predictions, test_data$class_target)
# Clustering Example
# Assuming you want to perform k-means clustering on your dataset
# Select the columns for clustering (replace 'columns_for_clustering')
cluster_data <- your_data[, c('columns_for_clustering')]
# Perform k-means clustering with a specified number of clusters (e.g., 3)
kmeans_model <- kmeans(cluster_data, centers = 3)
# View cluster assignments
cluster_assignments <- kmeans_model$cluster
# Save the cluster assignments back to your data
your_data$cluster_group <- cluster_assignments
# Save the preprocessed dataset (optional)
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
df <- iris
head(df)
str(df)
summary(df)
View(df)
