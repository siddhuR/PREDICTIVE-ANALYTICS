getwd()
concrete = read.csv("Concrete_Data.csv")
concrete = read.csv("Concrete_Data.csv")
View(concrete)
str(concrete)
#total 9 variables are there. one is 'strength' dependent on all the
#other 8 variables.
#Neural networks work best when the input data are scaled to a
#narrow range around zero, and here, we see values ranging anywhere from zero
#up to over a thousand. So normalize the data
normalize <- function(x) {  return((x - min(x)) / (max(x) - min(x)))}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
summary(concrete_norm$strength)
summary(concrete$strength)
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
library(neuralnet)
concrete_model <- neuralnet(strength ~ cement + slag + ash + water
+ superplasticizer
+ coarseagg + fineagg + age,
data = concrete_train)
#total 9 variables are there. one is 'strength' dependent on all the
#other 8 variables.
#Neural networks work best when the input data are scaled to a
#narrow range around zero, and here, we see values ranging anywhere from zero
#up to over a thousand. So normalize the data
hist(concrete$strength)
concrete_test <- concrete_norm[774:1030, ]
library(neuralnet)
concrete_model <- neuralnet(strength ~ cement + slag + ash + water
+ superplasticizer
+ coarseagg + fineagg + age,
data = concrete_train)
plot(concrete_model)
model_results = compute(concrete_model,concrete_test[1:8])
#It returns a list with two components: $neurons, which stores the
#neurons for each layer in the network, and
#$net.result, which stores the predicted values.
predicted_strength <- model_results$net.result
cor(predicted_strength, concrete_test$strength)
concrete_model2 <- neuralnet(strength ~ cement + slag + ash + water + superplasticizer + coarseagg + fineagg + age,
data = concrete_train, hidden = 5)
plot(concrete_model2)
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
library(neuralnet)
library(MASS)
Boston
View(Boston)
help("Boston")
DataFrame = Boston
str(DataFrame)
hist(DataFrame$medv)
dim(DataFrame)
head(DataFrame)
maxValue = apply(DataFrame, 2,max)
#Returns a vector or array or list of values obtained by applying a function
#to margins of an array or matrix. 1 indicates
#rows and 2 indicates column
maxValue
minValue = apply(DataFrame, 2,min)
minValue
DataFrame = as.data.frame( scale(DataFrame,center = minValue,
scale = maxValue-minValue))
#Returns a vector or array or list of values obtained by applying a
#function to margins of an array or matrix.
ind = sample(1:nrow(DataFrame),400)
trainDF = DataFrame[ind,]
testDF = DataFrame[-ind,]
allVars = colnames(DataFrame)
predictorVars = allVars[!allVars%in% "medv"]
predictorVars
predictorVars = paste(predictorVars, collapse =  "+")
predictorVars
form = as.formula(paste("medv~", predictorVars, collapse = "+"))
form
neuralMOdel = neuralnet(formula = form, hidden = c(4,2),
linear.output = T,data = trainDF)
plot(neuralMOdel)
# Importing the dataset
dataset = read.csv('Position_Salaries.csv')
# Importing the dataset
getwd()
dataset = read.csv('Position_Salaries.csv')
dataset = read.csv('Position_Salaries.csv')
# Importing the dataset
getwd()
dataset = read.csv('Position_Salaries.csv')
dataset = read.csv("Position_Salaries.csv")
# Importing the dataset
getwd()
dataset = read.csv("Position_Salaries.csv")
dataset = dataset[2:3]
dataset
# Fitting Linear Regression to the dataset
lin_reg = lm(formula = Salary ~ .,
data = dataset)
summary(lin_reg)
# Fitting Polynomial Regression to the dataset
dataset$Level2 = dataset$Level^2
dataset$Level3 = dataset$Level^3
dataset$Level4 = dataset$Level^4
poly_reg = lm(formula = Salary ~ .,
data = dataset)
summary(poly_reg)
dataset$Level
dataset$Level2
dataset$Level3
dataset$Level4
summary(lin_reg)
summary(poly_reg)
# Visualising the Linear Regression results
# install.packages('ggplot2')
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(lin_reg, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (Linear Regression)') +
xlab('Level') +
ylab('Salary')
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary),
colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(poly_reg, newdata = dataset)),
colour = 'blue') +
ggtitle('Truth or Bluff (Polynomial Regression)') +
xlab('Level') +
ylab('Salary')
# Predicting a new result with Linear Regression
predict(lin_reg, data.frame(Level = 6.5))
# Predicting a new result with Polynomial Regression
predict(poly_reg, data.frame(Level = 6.5,
Level2 = 6.5^2,
Level3 = 6.5^3,
Level4 = 6.5^4))
getwd()
concrete = read.csv("Concrete_Data.csv")
View(concrete)
str(concrete)
#total 9 variables are there. one is 'strength' dependent on all the
#other 8 variables.
#Neural networks work best when the input data are scaled to a
#narrow range around zero, and here, we see values ranging anywhere from zero
#up to over a thousand. So normalize the data
hist(concrete$strength)
normalize <- function(x) {  return((x - min(x)) / (max(x) - min(x)))}
concrete_norm <- as.data.frame(lapply(concrete, normalize))
summary(concrete_norm$strength)
summary(concrete$strength)
concrete_train <- concrete_norm[1:773, ]
concrete_test <- concrete_norm[774:1030, ]
library(neuralnet)
concrete_model <- neuralnet(strength ~ cement + slag + ash + water
+ superplasticizer
+ coarseagg + fineagg + age,
data = concrete_train)
plot(concrete_model)
model_results = compute(concrete_model,concrete_test[1:8])
#It returns a list with two components: $neurons, which stores the
#neurons for each layer in the network, and
#$net.result, which stores the predicted values.
predicted_strength <- model_results$net.result
cor(predicted_strength, concrete_test$strength)
concrete_model2 <- neuralnet(strength ~ cement + slag + ash + water + superplasticizer + coarseagg + fineagg + age,
data = concrete_train, hidden = 5)
plot(concrete_model2)
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
library(MASS)
library(neuralnet)
set.seed(123)
DataFrame<-Boston
help("boston")
help("Boston")
str(DataFrame)
View(DataFrame)
hist(DataFrame)
hist(DataFrame$medv)
dim(DataFrame)
apply(DataFrame,2,range)
maxValue
#it will find the max value of every column
maxValue=apply(DataFrame,2,max)
maxValue
minValue=apply(DataFrame, 2, min)
minValue
#sacle function give mean = 0 and standard deviation =1 for each variable
DataFrame = as.data.frame(scale(DataFrame,center = minValue,
scale = maxValue-minValue))
DataFrame
#sacle function give mean = 0 and standard deviation =1 for each variable
DataFrame = as.data.frame(scale(DataFrame,center = minValue,
scale = maxValue-minValue))
x=sample(1:nrow(DataFrame),400)
x
train = DataFrame[x,]
test = DataFrame[-x,]
train
neuralmodel = neuralnet(medv ~.,hidden = c(4,2),linear.output = T,
data = train)
plot(neuralmodel)
plot(neuralmodel)
#in the model 13 are the input nodes. 2 hidden layers are there. first
#consist of 4 nodes and second consist of 2
plot(neuralmodel)
model_results = compute(neuralmodel,test[1:13])
str(model_results)
predicted <- model_results$net.result
cor(predicted, test$medv)
neuralmodel2 = neuralnet(medv ~.,hidden = c(8,4),linear.output = T,
data = train)
plot(neuralmodel2)
model_results = compute(neuralmodel2,test[1:13])
model_results2 = compute(neuralmodel2,test[1:13])
predicted2 <- model_results$net.result
cor(predicted2, test$medv)
neuralmodel2 = neuralnet(medv ~.,hidden = c(6,3),linear.output = T,
data = train)
plot(neuralmodel2)
model_results2 = compute(neuralmodel2,test[1:13])
predicted2 <- model_results$net.result
cor(predicted2, test$medv)
neuralmodel2 = neuralnet(medv ~.,hidden = 5,linear.output = T,
data = train)
plot(neuralmodel2)
model_results2 = compute(neuralmodel2,test[1:13])
predicted2 <- model_results$net.result
cor(predicted2, test$medv)
neuralmodel2 = neuralnet(medv ~.,hidden = 1,linear.output = T,
data = train)
plot(neuralmodel2)
model_results2 = compute(neuralmodel2,test[1:13])
predicted2 <- model_results$net.result
cor(predicted2, test$medv)
neuralmodel2 = neuralnet(medv ~.,hidden = 2,linear.output = T,
data = train)
plot(neuralmodel2)
model_results2 = compute(neuralmodel2,test[1:13])
predicted2 <- model_results$net.result
cor(predicted2, test$medv)
neuralmodel = neuralnet(medv ~.,hidden = c(4,2),linear.output = T,
data = train)
#in the model 13 are the input nodes. 2 hidden layers are there. first
#consist of 4 nodes and second consist of 2
plot(neuralmodel)
model_results = compute(neuralmodel,test[1:13])
str(model_results)
predicted <- model_results$net.result
cor(predicted, test$medv)
library(MASS)
#Boston dataset is present in MASS package
library(neuralnet)
#set seed so that we can get same values everytime
set.seed(123)
#storing boston dataset
DataFrame<-Boston
#to get the details of Boston
help("Boston")
str(DataFrame)
hist(DataFrame$medv)
dim(DataFrame)
#range means mix and max values of columns
apply(DataFrame,2,range)
#it will find the max value of every column
maxValue=apply(DataFrame,2,max)
maxValue
#It will find the min value for every column
minValue=apply(DataFrame, 2, min)
minValue
#sacle function give mean = 0 and standard deviation =1 for each variable
DataFrame = as.data.frame(scale(DataFrame,center = minValue,
scale = maxValue-minValue))
#used to create sample of 400 rows from 506 rows
x=sample(1:nrow(DataFrame),400)
x
train = DataFrame[x,] #400 rows
test = DataFrame[-x,] #106 rows
neuralmodel = neuralnet(medv ~.,hidden = c(4,2),linear.output = T,
data = train)
#in the model 13 are the input nodes. 2 hidden layers are there. first
#consist of 4 nodes and second consist of 2
plot(neuralmodel)
model_results = compute(neuralmodel,test[1:13])
str(model_results)
predicted <- model_results$net.result
cor(predicted, test$medv)
neuralmodel = neuralnet(medv ~.,hidden = c(4,2),
data = train)
#in the model 13 are the input nodes. 2 hidden layers are there. first
#consist of 4 nodes and second consist of 2
plot(neuralmodel)
model_results = compute(neuralmodel,test[1:13])
str(model_results)
predicted <- model_results$net.result
cor(predicted, test$medv)
dataset = read.csv('social.csv')
str(dataset)
View(dataset)
dataset = dataset[3:5]
View(dataset)
# Taking columns 3-5
#dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
#install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
View(split)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
View(training_set)
#scale is generic function whose default method centers and/or scales
#the columns of a numeric matrix.
test_set[-3] = scale(test_set[-3])
# Fitting SVM to the Training set
#install.packages('e1071')
library(e1071)
classifier = svm(formula = Purchased ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
cm
# Plotting the training data set results
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
X1
grid_set = expand.grid(X1, X2)
#expand.grid will create dataframe
grid_set
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1),
length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1',
'aquamarine'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
#Visualizing the test set results
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'SVM (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)),
add = TRUE)
#if true, then add to the current plot
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1',
'aquamarine'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set.seed(100)
x <- matrix(rnorm(40),20,2)
#it will create a mtrix of 40 normalized random numbers arranged in
#20 rows and 2 columns
x
y <- rep(c(-1,1),c(10,10))
#it will repeat -1 and 1 for 10-10 times
y
x[y==1,]
x[y == 1,] = x[y == 1,] + 1
x[y==1,]
plot(x, col = y + 3, pch = 19)
library(e1071)
data = data.frame(x, y = as.factor(y))
data
data.svm = svm(y ~ ., data = data, kernel = "linear", cost = 10,
scale = FALSE)
print(data.svm)
plot(data.svm, data)
data=iris
str(data)
summary(data)
library(caTools)
set.seed(123)
split = sample.split(data$Species, SplitRatio = 0.75)
training_set = subset(data, split == TRUE)
test_set = subset(data, split == FALSE)
library(e1071)
classifier = svm(formula = Species ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
y_pred = predict(classifier, newdata = test_set[-5])
plot(classifier, training_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, test_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
cm = table(test_set[, 5], y_pred)
cm
1 - sum(diag(cm)) / sum(cm)
cm = table(test_set[, 5], y_pred)
cm
plot(classifier, training_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
classifier = svm(formula = Species ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
y_pred = predict(classifier, newdata = test_set[-5])
plot(classifier, training_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, test_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, training_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, test_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
cm = table(test_set[, 5], y_pred)
cm
data=iris
str(data)
summary(data)
library(caTools)
set.seed(123)
split = sample.split(data$Species, SplitRatio = 0.75)
training_set = subset(data, split == TRUE)
test_set = subset(data, split == FALSE)
library(e1071)
classifier = svm(formula = Species ~ .,
data = training_set,
type = 'C-classification',
kernel = 'linear')
y_pred = predict(classifier, newdata = test_set[-5])
plot(classifier, training_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
plot(classifier, test_set, Petal.Width ~ Petal.Length,
slice = list(Sepal.Width = 3, Sepal.Length = 4))
cm = table(test_set[, 5], y_pred)
cm
