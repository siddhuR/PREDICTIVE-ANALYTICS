library(caTools)
str(dataset)
dataset = dataset[3:5]
dataset$Purchased = factor(dataset$Purchased, levels=c(0,1))
set.seed(123)
dataset <- read.csv("social.csv")
dataset<-read.csv("Datasets/social.csv")
str(dataset)
dataset= dataset[3:5]
View(dataset)
dataset$Purchased <- factor(dataset$Purchased, levels=c(0,1))
install.packages('caTools')
install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set= subset(dataset,split==TRUE)
test_set <- subset(dataset,split==FALSE)
#normalization
training_set[-3] <- scale(training_set[-3])
View(training_set)
test_set[-3] = scale(test_set[-3])
library(e1073)
library(e1071)
classifier = svm(formula = purchased~,
classifier = svm(formula = purchased~.,
data = training_set,
type = 'c=classification',
kernel = 'linear')
classifier = svm(formula = purchased~.,
data = training_set,
type = 'c=classification',
kernel = 'linear')
classifier = svm(formula = purchased~.,
data = training_set,
type = 'c-classification',
kernel = 'linear')
classifier = svm(formula = Purchased~.,
data = training_set,
type = 'C-classification',
kernel = 'linear')
#Setting the path of directory
setwd("E:/Study/LPU/B.TECH/4th Year/7th Semester/INT234 - PREDICTIVE ANALYTICS/PREDICTIVE-ANALYTICS/R codes")
df<- read.csv("Datasets/sales dataset.csv")
View(df)
summary(df)
# Boxplot
library(ggplot2)
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Cost))+geom_boxplot()
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Qty))+geom_boxplot()
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Cost))+geom_boxplot()
ggplot(data = df, aes(x=factor(df$Item.Category),
y=df$Sales_Qty))+geom_boxplot()
#Preprocessing the sales data
df <- read.csv("Datasets/SalesDatafor preprocessing.csv")
str(df)
summary(df)
# Delete all rows with missing data and name the dataset as df1
df1 <- na.omit(df)
summary(df1)
# Replace the missing values with the mean value of each variable
df$Sales[is.na(df$Sales)] <- mean(df$Sales, na.rm = TRUE)
df$Profit[is.na(df$Profit)] <- mean(df$Profit, na.rm = TRUE)
df$Unit.Price[is.na(df$Unit.Price)] <- mean(df$Unit.Price, na.rm = TRUE)
summary(df)
df$Order.Priority[is.na(df$Order.Priority)] <- sample(levels(df$Order.Priority),
size = sum(is.na(df$Order.Priority)),
replace = TRUE)
# Replace the missing values with random value between min and max of each variable
df$Sales[is.na(df$Sales)] <- runif(sum(is.na(df$Sales)), min(df$Sales, na.rm = TRUE), max(df$Sales, na.rm = TRUE))
#Preprocessing the sales data
df <- read.csv("Datasets/SalesDatafor preprocessing.csv")
View(df)
str(df)
summary(df)
# Delete all rows with missing data and name the dataset as df1
df1 <- na.omit(df)
summary(df1)
# Replace the missing values with the mean value of each variable
df$Sales[is.na(df$Sales)] <- mean(df$Sales, na.rm = TRUE)
df$Profit[is.na(df$Profit)] <- mean(df$Profit, na.rm = TRUE)
df$Unit.Price[is.na(df$Unit.Price)] <- mean(df$Unit.Price, na.rm = TRUE)
summary(df)
df$Order.Priority[is.na(df$Order.Priority)] <- sample(levels(df$Order.Priority),
size = sum(is.na(df$Order.Priority)),
replace = TRUE)
# Load necessary libraries
library(dplyr)  # for data manipulation
library(tidyr)  # for data tidying
library(caret)  # for classification
library(caretEnsemble)  # for regression and classification
library(cluster)  # for clustering
# Load your dataset (replace 'your_dataset.csv' with the actual file path or data frame)
your_data <- read.csv("your_dataset.csv")
# Load necessary libraries
library(dplyr)  # for data manipulation
library(tidyr)  # for data tidying
library(caret)  # for classification
library(caretEnsemble)  # for regression and classification
library(cluster)  # for clustering
# Load your dataset (replace 'your_dataset.csv' with the actual file path or data frame)
your_data <- read.csv("your_dataset.csv")
# Classification Example
# Assuming you have a classification target variable named 'class_target'
# Split data into training and testing sets (70-30 split)
set.seed(123)  # for reproducibility
train_index <- createDataPartition(your_data$class_target, p = 0.7, list = FALSE)
train_data <- your_data[train_index, ]
test_data <- your_data[-train_index, ]
# Train a classification model (e.g., Random Forest)
rf_model <- train(class_target ~ ., data = train_data, method = "rf")
# Make predictions
predictions <- predict(rf_model, test_data)
# Evaluate the classification model (e.g., confusion matrix and accuracy)
confusionMatrix(predictions, test_data$class_target)
# Clustering Example
# Assuming you want to perform k-means clustering on your dataset
# Select the columns for clustering (replace 'columns_for_clustering')
cluster_data <- your_data[, c('columns_for_clustering')]
# Perform k-means clustering with a specified number of clusters (e.g., 3)
kmeans_model <- kmeans(cluster_data, centers = 3)
# View cluster assignments
cluster_assignments <- kmeans_model$cluster
# Save the cluster assignments back to your data
your_data$cluster_group <- cluster_assignments
# Save the preprocessed dataset (optional)
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
# Save the preprocessed dataset (optional)
# write.csv(your_data, file = "preprocessed_data.csv")
df <- iris
head(df)
str(df)
summary(df)
View(df)
data("iris")
View(airquality)
head(iris)
summary(iris)
#NAs
colSums(is.na(iris))
df = iris %>% select(1:4) %>% scale() %>% data.frame()
#NAs
colSums(is.na(iris))
df = iris %>% select(1:4) %>% scale() %>% data.frame()
library(dplyr)
6df = iris %>% select(1:4) %>% scale() %>% data.frame()
6df = iris %>% select(1:4) %>% scale() %>% data.frame()
df = iris %>% select(1:4) %>% scale() %>% data.frame()
head(df)
result = kmeans(df, 3, nstart = 25)
print(result)
aggregate(df, by = list(cluster = result$cluster),mean)
#df = cbind(df, iris$Species)
newdata = cbind(df, cluster=result$cluster)
head(newdata)
result$cluster
head(result$cluster,4)
result$centers
result$size
#visulization
library("ggplot2")
library(factoextra)
ggplot(data = newdata, aes(x=result$cluster)) + geom_bar(fill = "steelblue")
library(factoextra)
ggplot(data = newdata, aes(x=result$cluster)) + geom_bar(fill = "steelblue")
#Cluster
fviz_cluster(result, data = newdata)
View(iris)
sum(is.na(iris))
library("ggplot2")
ggplot(iris, aes(x=Petal.Length,y=Petal.Width,color=Species)) +geom_point()
?kmeans
result<-kmeans(iris[,3:4],centers = 3,nstart = 25)
print(result)
result$cluster
head(result$cluster, 4)
result$size
result$centers
data("iris")
str(iris)
iris.new<- iris[,c(1,2,3,4)]
iris.class<- iris[,"Species"]
head(iris.new)
head(iris.class)
normalize <- function(x){
return ((x-min(x))/(max(x)-min(x)))
}
iris.new$Sepal.Length<- normalize(iris.new$Sepal.Length)
iris.new$Sepal.Width<- normalize(iris.new$Sepal.Width)
iris.new$Petal.Length<- normalize(iris.new$Petal.Length)
iris.new$Petal.Width<- normalize(iris.new$Petal.Width)
head(iris.new)
result<- kmeans(iris.new,3)
result$size
table(result$cluster,iris.class)
#install.packages("factoextra")
library("factoextra")
#install.packages("NbClust")
#library("NbClust")
fviz_nbclust(iris[,3:4],kmeans,method = "wss") +
geom_vline(xintercept = 3,linetype=5) + labs(subtitle="Elbow method")
#visulization
library("ggplot2")
library(factoextra)
ggplot(data = newdata, aes(x=result$cluster)) + geom_bar(fill = "steelblue")
#Cluster
fviz_cluster(result, data = newdata)
iris
iris1=iris1iris1
iris1=iris1
iris
iris
iris1=iris
d = dist(iris1, method = "euclidean")
iris1
d = dist(iris1, method = "euclidean")
iris1$Species = NULL
iris1
d = dist(iris1, method = "euclidean")
hclust?
hfit = hclust(d, method = "average")
plot(hfit)
hclust?
hfit = hclust(d, method = "average")
?hclust
hfit = hclust(d, method = "average")
plot(hfit)
cutree?
grps = cutree(hfit,k=4)
?cutree
grps = cutree(hfit,k=4)
grps
rect.hclust(hfit,k=4,border="red")
#start
install.packages("arulesViz")
install.packages("arules")
install.packages("arules")
library(datasets)
library(arules)
library(arules)
library(arulesViz)
#explore the data
data(Groceries)
class(Groceries)
Groceries@itemInfo[1:20,]
View(Groceries)
apply(Groceries@data[,10:20],2,function(r) paste(Groceries@itemInfo[r,"labels"]
, collapse=", "))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
summary(itemsets)
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target=
"frequent itemsets"))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,target= "frequent itemsets"))
library(datasets)
library(arules)
library(arulesViz)
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,
target= "frequent itemsets"))
summary(itemsets)
#explore the data
data(Groceries)
class(Groceries)
Groceries@itemInfo[1:20,]
View(Groceries)
apply(Groceries@data[,10:20],2,function(r) paste(Groceries@itemInfo[r,"labels"]
, collapse=", "))
itemsets<-apriori(Groceries,parameter=list(minlen=1,maxlen=1,support=0.02,
target= "frequent itemsets"))
summary(itemsets)
inspect(head(sort(itemsets,by="support"),10))
itemsets<-apriori(Groceries,parameter=list(minlen=2,maxlen=2,support=0.02,
target="frequent itemsets"))
#start
install.packages("arulesViz")
install.packages("arules")
install.packages("arules")
install.packages("arules")
install.packages("arules")
sms_raw<- read.csv("Datasets\sms_spam")
sms_raw<- read.csv("Datasets/sms_spam")
sms_raw<- read.csv("Datasets/sms_spam.csv")
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$text)
#choose sms_spam dataset
sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
#choose sms_spam dataset
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
sms_raw<-read.csv("Datasets/sms_spam")
#choose sms_spam dataset
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
sms_raw<-read.csv("Datasets/sms_spam")
#choose sms_spam dataset
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
sms_raw<-read.csv("Datasets/sms_spam")
#choose sms_spam dataset
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
df<- read.csv("Datasets/sms_spam")
#choose sms_spam dataset
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
getwd()
df<- read.csv("Datasets/sms_spam.csv")
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
#install.packages("tm")
library(tm)
install.packages("tm")
#sms_raw <- read.csv(file.choose(),sep=",",header=T,stringsAsFactors = FALSE)
df<- read.csv("Datasets/sms_spam.csv")
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
#install.packages("tm")
library(tm)
# First step for text mining is to create corpus,
#which is the collection of text documents
# types of corpus - Vcorpus, Pcorpus
#VectorCorpus reader function which used to create source object
#from existing vector
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
inspect(sms_corpus[1:2])
as.character(sms_corpus[[1]])
lapply(sms_corpus[1:2], as.character)
sms_corpus_clean <- tm_map(sms_corpus,
content_transformer(tolower))
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
sms_corpus_clean
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
sms_corpus_clean <- tm_map(sms_corpus_clean,
removeWords, stopwords())
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
#install.packages("SnowballC")
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
View(sms_dtm)
#######word cloud chart######
#install.packages("wordcloud")
library("wordcloud")
#######word cloud chart######
#install.packages("wordcloud")
library("wordcloud")
wordcloud(sms_corpus_clean,min.freq = 150,random.order = F)
#install.packages("RColorBrewer")
library("RColorBrewer")
wordcloud(sms_corpus_clean,min.freq = 150,random.order = F,
color=brewer.pal(8,"Dark2"),rot.per = 0.20)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
#install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
findFreqTerms(sms_dtm_train, 5)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2,
convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2,
convert_counts)
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
wordcloud(sms_corpus_clean,min.freq = 150,random.order = F,
color=brewer.pal(8,"Dark2"),rot.per = 0.20)
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
#install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
findFreqTerms(sms_dtm_train, 5)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2,
convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2,
convert_counts)
#install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE,
dnn = c('predicted', 'actual'))
getwd()
read.csv("TwitterDataset.csv")
read.csv("Datasets/TwitterDataset.csv")
df <- read.csv("Datasets/TwitterDataset.csv")
df
df
View(df)
View(df)
twitter <- read.csv("Datasets/TwitterDataset.csv")
View(twitter)
subject_name<- c("John Doe", "Jane Doe", "Steve Graves") #vector to store patient name
temperature<- c(98.1, 98.6, 101.4) #vector to store temperature of patient
flu_status <- c(FALSE, FALSE, TRUE) #vector to store status of flu
temperature[2] #print temp of Jane Doe
temperature[2:3] #print temperature with in certain range
temperature[-2] #exclude the temperature of the second patient
temperature[c(TRUE, TRUE, FALSE)] #print the temperature of those pateints where value is TRUE
# Elements are arranged sequentially by row.
M <- matrix(c(3:14), nrow = 4, byrow = TRUE)
print(M)
# Elements are arranged sequentially by column.
N <- matrix(c(3:14), nrow = 4, byrow = FALSE)
print(N)
# Define the column and row names.
rownames = c("row1", "row2", "row3", "row4")
colnames = c("col1", "col2", "col3")
P <- matrix(c(3:14), nrow = 4, byrow = TRUE, dimnames = list(rownames, colnames))
print(P)
vector1 <- c(5,9,3)
vector2 <- c(10,11,12,13,14,15)
column.names <- c("COL1","COL2","COL3")
row.names <- c("ROW1","ROW2","ROW3")
matrix.names <- c("Matrix1","Matrix2")
# Take these vectors as input to the array.
result <- array(c(vector1,vector2),dim = c(3,3,2),dimnames = list(row.names,column.names,
matrix.names))
print(result)
usedcars <- read.csv("usedcars.csv", stringsAsFactors = FALSE)
usedcars <- read.csv("usedcars.csv", stringsAsFactors = FALSE)
str(usedcars)
#Decision Tree using Party Package
#Classes =Normal, Suspect, Pathologic. NSP variable is representing classes. It is a categorical variable
getwd()
data <- read.csv("Cardiotocographic.csv",stringsAsFactors = FALSE )
setwd("E:/Study/LPU/B.TECH/4th Year/7th Semester/INT234 - PREDICTIVE ANALYTICS/PA Programs2023/PE Programs2022/Unit 2")
data <- read.csv("Cardiotocographic.csv",stringsAsFactors = FALSE )
str(data)
View(data)
str(data)
data$NSPF<-factor(data$NSP)#integer variable will be converted into factor
str(data) #a new variable now added into existing dataset
#Training and Testing Data
set.seed(1234)
#Training and Testing Data
set.seed(1234)
pd<-sample(2,nrow(data), replace = TRUE, prob = c(0.8,0.2)) #sample of size 2, no of rows as equal to data
#replacement as TRUE and probobability of sample as 80% training and 20% testing
train<-data[pd==1,]
test<-data[pd==2,]
#Decision Tree
#install.packages("party")
library(party)
tree<-ctree(NSPF~LB+AC+FM,data = train, controls = ctree_control
(mincriterion = 0.90, minsplit = 200))
#using LB, AC, FM to classify data. controls is a parameter to control the length of the tree. mincriterian
#is the confidence level. It means that 90% confidence is there
#that a variable is significant.minsplit is
#200 means that a tree will split into 2 when the sample size is atleast 200
tree
plot(tree)
#prediction
predict(tree,test,type="prob")
predict(tree,test)
#Decision Tree with rpart package
library(rpart)
tree1 <-rpart(NSPF ~ LB+AC+FM, train)
library(rpart.plot)
rpart.plot(tree1)
predict(tree1,test)
